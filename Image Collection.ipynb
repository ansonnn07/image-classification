{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory to save your images into\n",
    "IMAGE_DIR = \"dataset\"\n",
    "\n",
    "# defining the class names for the dataset\n",
    "CLASS_NAMES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \n",
    "               \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n",
    "               \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\n",
    "\n",
    "# define the number of images to be collected for each class\n",
    "# set at least 30 images each class, up to you\n",
    "IMAGES_PER_CLASS = 30\n",
    "\n",
    "# Set the image size that we want to save them with\n",
    "IMAGE_SIZE = (320, 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the config for the things to display by OpenCV\n",
    "\n",
    "# Set position of the text to show on feed\n",
    "TEXT_POS = (30, 30)\n",
    "# Position to show the counter of image number for each label\n",
    "COUNTER_POS = (30, 60)\n",
    "FONT_SCALE = 0.6\n",
    "TEXT_COLOR = (0, 255, 0)\n",
    "TEXT_THICKNESS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frames to capture per second\n",
    "# Adjust this to capture images faster or slower every second\n",
    "FPS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next cell defines the Region of interest (ROI) to capture during the recording.\n",
    "- We define this region to crop out specific region from the feed to save as image\n",
    "- The definition of the 4 numbers: \n",
    "    - `startX` = x-coordinate of top left\n",
    "    - `endX` = x-coordinate of bottom right\n",
    "    - `startY` = y-coordinate of top left\n",
    "    - `endY` = y-coordinate of bottom right\n",
    "- Make sure to set the dimensions to be square to make things easier\n",
    "- E.g. (320, 620, 100, 400) would result in 300 x 300 dimensions\n",
    "\n",
    "NOTE: This ROI should be based on your webcam's resolution, make sure not to exceed\n",
    "the width and height of the webcam's resolution. But generally, OpenCV will set the\n",
    "default width and height to be (640, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTURE_ROI = (320, 620, 100, 400)\n",
    "startX, endX, startY, endY = CAPTURE_ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture Image from Webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`label_arr` is created to know which point of index represents which label's turn to capture image.\n",
    "\n",
    "Basically, every label will have 3 different stages (or turns) for 3 different actions:\n",
    "- a stage for standby (wait to start capture)\n",
    "- a stage for capturing train set images\n",
    "- a stage for capturing test set images\n",
    "\n",
    "Therefore this `label_arr` is an array to keep track of the stages (indices) of which class label should be used. Once you try checking and running the code blocks below then you will understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'A' 'A' 'B' 'B' 'B' 'C' 'C' 'C' 'D' 'D' 'D' 'E' 'E' 'E' 'F' 'F' 'F'\n",
      " 'G' 'G' 'G' 'H' 'H' 'H' 'I' 'I' 'I' 'K' 'K' 'K' 'L' 'L' 'L' 'M' 'M' 'M'\n",
      " 'N' 'N' 'N' 'O' 'O' 'O' 'P' 'P' 'P' 'Q' 'Q' 'Q' 'R' 'R' 'R' 'S' 'S' 'S'\n",
      " 'T' 'T' 'T' 'U' 'U' 'U' 'V' 'V' 'V' 'W' 'W' 'W' 'X' 'X' 'X' 'Y' 'Y' 'Y']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "label_arr = np.empty(len(CLASS_NAMES) * 3, dtype=str)\n",
    "total_steps = len(label_arr)\n",
    "\n",
    "for label in CLASS_NAMES:\n",
    "    for idx in range(i, i+3):\n",
    "        label_arr[idx] = label\n",
    "    i += 3\n",
    "\n",
    "print(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_standby():\n",
    "    # get the current label based on the current stage (or index)\n",
    "    current_label = label_arr[i]\n",
    "    # show the standby text on the frame\n",
    "    cv2.putText(copy, f\"({current_label}) Hit 'Enter' to record when ready\",\n",
    "                TEXT_POS, cv2.FONT_HERSHEY_COMPLEX,\n",
    "                FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS\n",
    "    )\n",
    "    \n",
    "def run_capture(data=\"Train\"):\n",
    "    \"\"\"\n",
    "    A function to show what class label is being captured now,\n",
    "    and run capturing images for a specific label, and count the number of images that have \n",
    "    been captured for the class, either for training set or test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # add the image count for the current class and data (train/test)\n",
    "    global image_count\n",
    "    image_count += 1\n",
    "    \n",
    "    # get the current label from label_arr that we created above to\n",
    "    # keep track of the class label for current stage\n",
    "    current_label = label_arr[i]\n",
    "    \n",
    "    # display on the feed what class we are capturing, and for which data (train/test)\n",
    "    cv2.putText(copy, f\"Capturing '{current_label}' gesture - {data} set\",\n",
    "                TEXT_POS, cv2.FONT_HERSHEY_COMPLEX,\n",
    "                FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS\n",
    "    )\n",
    "    # display the image counter on the feed\n",
    "    cv2.putText(copy, f\"Images captured: {image_count}\", COUNTER_POS,\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS\n",
    "    )\n",
    "    \n",
    "    # get the specific directory for our label\n",
    "    gesture_dir = os.path.join(IMAGE_DIR, data.lower(), current_label)\n",
    "    if not os.path.exists(gesture_dir):\n",
    "        # create the directory if not exists\n",
    "        os.makedirs(gesture_dir)\n",
    "    \n",
    "    # save the image with a specific name based on the class name and image count\n",
    "    image_path = os.path.join(gesture_dir, f\"{current_label}_{image_count}.jpg\")\n",
    "    cv2.imwrite(image_path, roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up the different points for each different stage\n",
    "\n",
    "# the points where we show a standby feed to press 'ENTER' to start capturing\n",
    "standby_points = np.arange(0, total_steps, 3)\n",
    "# the points where we run capturing images for train set\n",
    "train_points = np.arange(1, total_steps, 3)\n",
    "# the points where we run capturing images for test set\n",
    "test_points = np.arange(2, total_steps, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run the code block below, OpenCV will open up 3 windows, and you should drag them and rearrange them side by side to see clearly what's going on.\n",
    "\n",
    "The window showing the grayscaled ROI is the frame that we will save as our image, you may choose to not convert them to grayscale if deemed necessary, as RGB color could be an important feature in many cases. In this case, RGB color is not a distinguishing feature for recognizing alphabets from gestures.\n",
    "\n",
    "Keep pressing the 'ENTER' key when you want to proceed to next stage every time. Or you may press 'ESC' key to exit any time. But you will have incomplete dataset saved in your directories. You may want to remove the incomplete dataset if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CAPTURE IMAGES\n",
    "\n",
    "# open the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# initialize the stage index from zero\n",
    "i = 0\n",
    "# initialize image counter\n",
    "image_count = 0\n",
    "\n",
    "# keep running while not finish running all the required capturing steps\n",
    "while i <= total_steps:\n",
    "    try:\n",
    "        # get the frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            # if couldn't access the webcam, stop running\n",
    "            break\n",
    "        \n",
    "        # flip the frame horizontally to make it easier to see\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # extract the ROI that we want to save as image\n",
    "        roi = frame[startY:endY, startX:endX]\n",
    "        # show the ROI side by side\n",
    "        cv2.imshow('ROI', roi)\n",
    "        \n",
    "        # create a grayscale ROI that we want to save.\n",
    "        # you may want to remove this conversion to grayscale if you think RGB color\n",
    "        #  is a very important feature to distinguish between classes,\n",
    "        #  which is not the case for alphabet classification\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # resize the ROI to a specific size we want to save the images with\n",
    "        roi = cv2.resize(roi, IMAGE_SIZE, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "        # show the scaled and grayscale ROI side by side\n",
    "        cv2.imshow('ROI scaled and gray', roi)\n",
    "        \n",
    "        # draw a rectangle box on the frame to show where we should place our\n",
    "        # object into, i.e. our hand with specific gesture in this case\n",
    "        copy = frame.copy()\n",
    "        cv2.rectangle(copy, (startX, startY), (endX, endY), (255, 0, 0), 5)\n",
    "        \n",
    "        if i in standby_points:\n",
    "            # show the standby text on the frame\n",
    "            show_standby()\n",
    "        elif i in train_points:\n",
    "            # run capturing for train set\n",
    "            run_capture(data=\"Train\")\n",
    "        elif i in test_points:\n",
    "            # run capturing for test set\n",
    "            run_capture(data=\"Test\")\n",
    "        elif i == total_steps:\n",
    "            # reached the end\n",
    "            cv2.putText(copy, \"Hit 'Enter' to exit\", TEXT_POS, cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS)\n",
    "        \n",
    "        # show the frame that has texts and ROI box on it\n",
    "        cv2.imshow('frame', copy)\n",
    "        \n",
    "        # wait for specific milliseconds for each frame\n",
    "        # basically controlling the approximate FPS\n",
    "        key = cv2.waitKey(int(1 / FPS * 1000))\n",
    "\n",
    "        if key == 13:\n",
    "            # press 'ENTER' to continue next run\n",
    "            # reset the image count for new class label\n",
    "            image_count = 0\n",
    "            # increment the point of time to proceed to next point\n",
    "            i += 1\n",
    "            \n",
    "        if key == 27:\n",
    "            # press 'ESC' to exit properly\n",
    "            break\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        # exit properly if user choose to interrupt the kernel\n",
    "        break\n",
    "\n",
    "# release the camera properly and destroy the OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if camera or OpenCV seems to not closing/functioning\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or collect from Google Search using a quick method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the [Fatkun Batch Download Image extension](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en) from Google Chrome.\n",
    "2. Open up a tab and search for the images you want, e.g. cloth face mask\n",
    "3. Then use the extension to download all the images that are found in the Google Search tab. Beware that the more you scroll down, the more images will be downloaded.\n",
    "\n",
    "NOTE: This requires you to perform more cleaning up and filtering to make sure the images that you've collected are of good quality! As the saying goes for machine learning, \"Garbage in, Garbage Out\". Good data is the core of a machine learning model with good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL - Compress them for Colab Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at dataset\\archive.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ARCHIVE_PATH = os.path.join(IMAGE_DIR, \"archive.tar.gz\")\n",
    "\n",
    "TRAIN_PATH = os.path.join(IMAGE_DIR, \"train\")\n",
    "TEST_PATH = os.path.join(IMAGE_DIR, \"test\")\n",
    "\n",
    "# add the training and testing datasets to a tar file\n",
    "!tar -czf {ARCHIVE_PATH} {TRAIN_PATH} {TEST_PATH}\n",
    "print(f\"File saved at {ARCHIVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the YouTube video below for the inspiring this notebook.\n",
    "1. https://www.youtube.com/watch?v=YjnGou4skGU - 36 Building Your Own Gesture Recognition System with Your Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
