{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory to save your images into\n",
    "DATASET_DIR = \"dataset\"\n",
    "\n",
    "# define the directory to save all images\n",
    "IMAGE_DIR = os.path.join(DATASET_DIR, 'all_images')\n",
    "# create it if not exists yet\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    os.makedirs(IMAGE_DIR)\n",
    "    print(f\"Created folder of {IMAGE_DIR}\")\n",
    "\n",
    "# defining the class names for the dataset\n",
    "CLASS_NAMES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \n",
    "               \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n",
    "               \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\n",
    "\n",
    "# define the number of images to be collected for each class\n",
    "# set at least 100 images each class, the more the better, up to you\n",
    "IMAGES_PER_CLASS = 500\n",
    "\n",
    "# Set this to true if you want to automatically limit the \n",
    "#  number of images for each class, otherwise you can just\n",
    "#  manually stop the recording every time later,\n",
    "# NOTE: Make sure the number of images for each class is balanced (almost the same number),\n",
    "#  imbalanced dataset is a common problem in classification tasks that\n",
    "#  we want to avoid\n",
    "LIMIT = False\n",
    "\n",
    "# Set the image size that we want to save them with\n",
    "IMAGE_SIZE = (320, 320)\n",
    "\n",
    "# Frames to capture per second\n",
    "# Adjust this to capture images faster or slower every second\n",
    "FPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the config for the things to display by OpenCV\n",
    "\n",
    "# Set position of the text to show on feed\n",
    "TEXT_POS = (15, 20)\n",
    "# Position to show the counter of image number for each label\n",
    "COUNTER_POS = (15, 60)\n",
    "FONT_SCALE = 0.6\n",
    "TEXT_COLOR = (0, 255, 0)\n",
    "TEXT_THICKNESS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next cell defines the Region of interest (ROI) to capture during the recording.\n",
    "- We define this region to crop out specific region from the feed to save as image\n",
    "- The definition of the 4 numbers: \n",
    "    - `startX` = x-coordinate of top left\n",
    "    - `endX` = x-coordinate of bottom right\n",
    "    - `startY` = y-coordinate of top left\n",
    "    - `endY` = y-coordinate of bottom right\n",
    "- Make sure to set the dimensions to be square to make things easier\n",
    "- E.g. (320, 620, 100, 400) would result in 300 x 300 dimensions\n",
    "\n",
    "NOTE: This ROI should be based on your webcam's resolution, make sure not to exceed\n",
    "the width and height of the webcam's resolution. But generally, OpenCV will set the\n",
    "default width and height to be (640, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTURE_ROI = (320, 620, 100, 400)\n",
    "startX, endX, startY, endY = CAPTURE_ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may check the ROI box position by running the cell below. This ROI region will be cropped out to be saved as our image later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verifying webcam is working, and the position of the ROI is ok\n",
    "# Press 'ESC' key to exit\n",
    "\n",
    "# open the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    # flip it horizontally to see clearly\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # draw the ROI box\n",
    "    cv2.rectangle(frame, (startX, startY), (endX, endY), (180, 0, 0), 3)\n",
    "    cv2.putText(frame, \"Press 'ESC' key to exit\",\n",
    "                TEXT_POS, cv2.FONT_HERSHEY_COMPLEX,\n",
    "                FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS\n",
    "    )\n",
    "    cv2.putText(frame, \"ROI\", (startX + 5, startY + 22),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS\n",
    "    )\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        # press 'ESC' to exit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You don't have to setup your own folders if you are collecting images using the webcam method in this notebook. If you are planning to collect images with other methods, then you should follow the approach explained in this section here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, the simplest approach is to structure your dataset into one folder for one each class.\n",
    "- For example, for a dogs VS cats dataset, the file structure would be as follow:\n",
    "\n",
    "```\n",
    "dataset\n",
    "├───dogs\n",
    "└───cats\n",
    "```\n",
    "- Therefore, for our dataset, there will be one folder per alphabet except for \"J\" and \"Z\", which are not used in the dataset as they require specific gestures (movements of hand) in order to determine the alphabets, hence there are 24 folders in total (26 alphabets minus 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture Image from Webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`label_arr` is created to know which point of index represents which class label's turn to capture image.\n",
    "\n",
    "Basically, every class will have 2 different stages (or turns) for 2 different actions:\n",
    "- a stage for standby (wait to start capture)\n",
    "- a stage for capturing images\n",
    "\n",
    "Therefore this `label_arr` is an array to keep track of the stages (point of time) of which class label should be used. Once you try checking and running the code blocks below then you will understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'A' 'B' 'B' 'C' 'C' 'D' 'D' 'E' 'E' 'F' 'F' 'G' 'G' 'H' 'H' 'I' 'I'\n",
      " 'K' 'K' 'L' 'L' 'M' 'M' 'N' 'N' 'O' 'O' 'P' 'P' 'Q' 'Q' 'R' 'R' 'S' 'S'\n",
      " 'T' 'T' 'U' 'U' 'V' 'V' 'W' 'W' 'X' 'X' 'Y' 'Y']\n"
     ]
    }
   ],
   "source": [
    "# initialize the stage_number\n",
    "i = 0\n",
    "# set the total number of stages for each class\n",
    "n_stages = 2\n",
    "label_arr = np.empty(len(CLASS_NAMES) * n_stages, dtype=str)\n",
    "total_steps = len(label_arr)\n",
    "\n",
    "for label in CLASS_NAMES:\n",
    "    for idx in range(i, i+n_stages):\n",
    "        label_arr[idx] = label\n",
    "    i += n_stages\n",
    "\n",
    "print(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_standby():\n",
    "    # get the current label based on the current stage (or index)\n",
    "    current_label = label_arr[i]\n",
    "    # show the standby text on the frame\n",
    "    cv2.putText(copy, f\"({current_label}) Hit 'Enter' to record when ready\",\n",
    "                TEXT_POS, cv2.FONT_HERSHEY_COMPLEX,\n",
    "                FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS\n",
    "    )\n",
    "    \n",
    "def run_capture():\n",
    "    \"\"\"\n",
    "    A function to show what class label is being captured now,\n",
    "    and run capturing images for a specific label, and count the number of images that have \n",
    "    captured for the class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # add the image count for the current class\n",
    "    global image_count\n",
    "    image_count += 1\n",
    "    \n",
    "    # get the current label from label_arr that we created above to\n",
    "    # keep track of the class label for current stage\n",
    "    current_label = label_arr[i]\n",
    "    \n",
    "    # display on the feed what class we are capturing\n",
    "    cv2.putText(copy, f\"Capturing class '{current_label}'\",\n",
    "                TEXT_POS, cv2.FONT_HERSHEY_COMPLEX,\n",
    "                FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS\n",
    "    )\n",
    "    # display the image counter on the feed\n",
    "    cv2.putText(copy, f\"Images captured: {image_count}\", COUNTER_POS,\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS\n",
    "    )\n",
    "    \n",
    "    # get the specific directory for our label\n",
    "    gesture_dir = os.path.join(IMAGE_DIR, current_label)\n",
    "    if not os.path.exists(gesture_dir):\n",
    "        # create the directory if not exists\n",
    "        os.makedirs(gesture_dir)\n",
    "    \n",
    "    # save the image with a specific name based on the class name and image count\n",
    "    image_path = os.path.join(gesture_dir, f\"{current_label}_{image_count}.jpg\")\n",
    "    cv2.imwrite(image_path, roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up the different points for each different stage\n",
    "#  They are set up like this so that we can alternate 3 stages for each class label\n",
    "\n",
    "# the points of time where we show a standby feed to press 'ENTER' to start capturing\n",
    "standby_points = np.arange(0, total_steps, n_stages)\n",
    "# the points of time where we run capturing images\n",
    "capture_points = np.arange(1, total_steps, n_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run the code block below, OpenCV will open up 3 windows, and you should drag them and rearrange them side by side to see clearly what's going on.\n",
    "\n",
    "The window showing the grayscaled ROI is the frame that we will save as our image, you may choose to not convert them to grayscale if deemed necessary, as RGB color could be an important feature in many cases. In this case, RGB color is not a distinguishing feature for recognizing alphabets from gestures.\n",
    "\n",
    "Keep pressing the 'ENTER' key when you want to proceed to next stage every time. Or you may press 'ESC' key to exit any time before completing all the class labels, but you will have incomplete dataset saved in your directories. You may want to remove the incomplete dataset if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CAPTURE IMAGES\n",
    "\n",
    "# open the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "# set up the frametime according to the FPS, in milliseconds\n",
    "frametime = int(1 / FPS * 1000)\n",
    "\n",
    "# initialize the stage index from zero\n",
    "i = 0\n",
    "# initialize image counter\n",
    "image_count = 0\n",
    "\n",
    "# keep running while not finish running all the required capturing steps\n",
    "try:\n",
    "    while i <= total_steps:\n",
    "        # get the frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            # if couldn't get any frame, stop running\n",
    "            break\n",
    "        \n",
    "        # flip the frame horizontally to make it easier to see\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # extract the ROI that we want to save as image\n",
    "        roi = frame[startY:endY, startX:endX]\n",
    "        # show the ROI side by side\n",
    "        cv2.imshow('ROI', roi)\n",
    "        \n",
    "        # create a grayscale ROI that we want to save.\n",
    "        # you may want to remove this conversion to grayscale if you think RGB color\n",
    "        #  is a very important feature to distinguish between classes,\n",
    "        #  which is not the case for alphabet classification\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # resize the ROI to a specific size we want to save the images with\n",
    "        roi = cv2.resize(roi, IMAGE_SIZE, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "        # show the scaled and grayscale ROI side by side\n",
    "        cv2.imshow('ROI scaled and gray', roi)\n",
    "        \n",
    "        # draw a rectangle box on the frame to show where we should place our\n",
    "        # object into, i.e. our hand with specific gesture in this case\n",
    "        copy = frame.copy()\n",
    "        cv2.rectangle(copy, (startX, startY), (endX, endY), (255, 0, 0), 5)\n",
    "        \n",
    "        if i in standby_points:\n",
    "            # show the standby text on the frame\n",
    "            show_standby()\n",
    "        elif i in capture_points:\n",
    "            # run capturing images\n",
    "            run_capture()\n",
    "        elif i == total_steps:\n",
    "            # reached the end\n",
    "            cv2.putText(copy, \"Hit 'Enter' to exit\", TEXT_POS, cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        FONT_SCALE, TEXT_COLOR, TEXT_THICKNESS)\n",
    "        \n",
    "        # show the frame that has texts and ROI box on it\n",
    "        cv2.imshow('frame', copy)\n",
    "        \n",
    "        # wait for specific milliseconds for each frame\n",
    "        # basically controlling the approximate FPS\n",
    "        key = cv2.waitKey(frametime)\n",
    "        \n",
    "        # press 'ENTER' to continue next run. \n",
    "        #  Or if LIMIT is set to True, it will automatically proceed to the\n",
    "        #  next stage when the IMAGES_PER_CLASS is reached\n",
    "        if key == 13 or (LIMIT and image_count == IMAGES_PER_CLASS):\n",
    "            # reset the image count for new class label\n",
    "            image_count = 0\n",
    "            # increment the point of time to proceed to next point\n",
    "            i += 1\n",
    "            \n",
    "        if key == 27:\n",
    "            # press 'ESC' to exit properly\n",
    "            break\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    # exit properly if user choose to interrupt the kernel\n",
    "    pass\n",
    "finally:\n",
    "    # release the camera properly and destroy the OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if camera or OpenCV seems to not closing/functioning\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or collect from Google Search using a quick method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the [Fatkun Batch Download Image extension](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en) from Google Chrome.\n",
    "2. Open up a tab and search for the images you want, e.g. cloth face mask\n",
    "3. Then use the extension to download all the images that are found in the Google Search tab. Beware that the more you scroll down, the more images will be downloaded.\n",
    "\n",
    "NOTE: This requires you to perform more cleaning up and filtering to make sure the images that you've collected are of good quality! As the saying goes for machine learning, \"Garbage in, Garbage Out\". Good data is the core of a machine learning model with good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset\\\\all_images'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the folder which we will find the images to split into 3 sets\n",
    "IMAGE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 8316\n",
      "Validation images: 924\n",
      "Test images: 3080\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# grab the paths to all input images in the original input directory\n",
    "# and shuffle them\n",
    "imagePaths = list(paths.list_images(IMAGE_DIR))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# compute the training and testing split\n",
    "i = int(len(imagePaths) * TRAIN_SPLIT)\n",
    "trainPaths = imagePaths[:i]\n",
    "testPaths = imagePaths[i:]\n",
    "\n",
    "# we'll be using part of the training data for validation\n",
    "i = int(len(trainPaths) * VAL_SPLIT)\n",
    "valPaths = trainPaths[:i]\n",
    "trainPaths = trainPaths[i:]\n",
    "\n",
    "print(\"Train images:\", len(trainPaths))\n",
    "print(\"Validation images:\", len(valPaths))\n",
    "print(\"Test images:\", len(testPaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\\training\n",
      "dataset\\validation\n",
      "dataset\\testing\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = os.path.join(DATASET_DIR, \"training\")\n",
    "VAL_PATH = os.path.join(DATASET_DIR, \"validation\")\n",
    "TEST_PATH = os.path.join(DATASET_DIR, \"testing\")\n",
    "print(TRAIN_PATH)\n",
    "print(VAL_PATH)\n",
    "print(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building 'training' split\n",
      "[INFO] 'creating dataset\\training' directory\n",
      "[INFO] 'creating dataset\\training\\I' directory\n",
      "[INFO] 'creating dataset\\training\\H' directory\n",
      "[INFO] 'creating dataset\\training\\M' directory\n",
      "[INFO] 'creating dataset\\training\\E' directory\n",
      "[INFO] 'creating dataset\\training\\Y' directory\n",
      "[INFO] 'creating dataset\\training\\D' directory\n",
      "[INFO] 'creating dataset\\training\\C' directory\n",
      "[INFO] 'creating dataset\\training\\O' directory\n",
      "[INFO] 'creating dataset\\training\\L' directory\n",
      "[INFO] 'creating dataset\\training\\V' directory\n",
      "[INFO] 'creating dataset\\training\\F' directory\n",
      "[INFO] 'creating dataset\\training\\B' directory\n",
      "[INFO] 'creating dataset\\training\\P' directory\n",
      "[INFO] 'creating dataset\\training\\W' directory\n",
      "[INFO] 'creating dataset\\training\\R' directory\n",
      "[INFO] 'creating dataset\\training\\T' directory\n",
      "[INFO] 'creating dataset\\training\\X' directory\n",
      "[INFO] 'creating dataset\\training\\Q' directory\n",
      "[INFO] 'creating dataset\\training\\U' directory\n",
      "[INFO] 'creating dataset\\training\\A' directory\n",
      "[INFO] 'creating dataset\\training\\G' directory\n",
      "[INFO] 'creating dataset\\training\\S' directory\n",
      "[INFO] 'creating dataset\\training\\N' directory\n",
      "[INFO] 'creating dataset\\training\\K' directory\n",
      "[INFO] building 'validation' split\n",
      "[INFO] 'creating dataset\\validation' directory\n",
      "[INFO] 'creating dataset\\validation\\S' directory\n",
      "[INFO] 'creating dataset\\validation\\U' directory\n",
      "[INFO] 'creating dataset\\validation\\A' directory\n",
      "[INFO] 'creating dataset\\validation\\L' directory\n",
      "[INFO] 'creating dataset\\validation\\C' directory\n",
      "[INFO] 'creating dataset\\validation\\W' directory\n",
      "[INFO] 'creating dataset\\validation\\H' directory\n",
      "[INFO] 'creating dataset\\validation\\O' directory\n",
      "[INFO] 'creating dataset\\validation\\F' directory\n",
      "[INFO] 'creating dataset\\validation\\Q' directory\n",
      "[INFO] 'creating dataset\\validation\\X' directory\n",
      "[INFO] 'creating dataset\\validation\\B' directory\n",
      "[INFO] 'creating dataset\\validation\\R' directory\n",
      "[INFO] 'creating dataset\\validation\\Y' directory\n",
      "[INFO] 'creating dataset\\validation\\T' directory\n",
      "[INFO] 'creating dataset\\validation\\P' directory\n",
      "[INFO] 'creating dataset\\validation\\M' directory\n",
      "[INFO] 'creating dataset\\validation\\G' directory\n",
      "[INFO] 'creating dataset\\validation\\K' directory\n",
      "[INFO] 'creating dataset\\validation\\V' directory\n",
      "[INFO] 'creating dataset\\validation\\N' directory\n",
      "[INFO] 'creating dataset\\validation\\E' directory\n",
      "[INFO] 'creating dataset\\validation\\D' directory\n",
      "[INFO] 'creating dataset\\validation\\I' directory\n",
      "[INFO] building 'testing' split\n",
      "[INFO] 'creating dataset\\testing' directory\n",
      "[INFO] 'creating dataset\\testing\\T' directory\n",
      "[INFO] 'creating dataset\\testing\\G' directory\n",
      "[INFO] 'creating dataset\\testing\\W' directory\n",
      "[INFO] 'creating dataset\\testing\\L' directory\n",
      "[INFO] 'creating dataset\\testing\\K' directory\n",
      "[INFO] 'creating dataset\\testing\\M' directory\n",
      "[INFO] 'creating dataset\\testing\\E' directory\n",
      "[INFO] 'creating dataset\\testing\\D' directory\n",
      "[INFO] 'creating dataset\\testing\\C' directory\n",
      "[INFO] 'creating dataset\\testing\\Q' directory\n",
      "[INFO] 'creating dataset\\testing\\A' directory\n",
      "[INFO] 'creating dataset\\testing\\N' directory\n",
      "[INFO] 'creating dataset\\testing\\P' directory\n",
      "[INFO] 'creating dataset\\testing\\V' directory\n",
      "[INFO] 'creating dataset\\testing\\S' directory\n",
      "[INFO] 'creating dataset\\testing\\R' directory\n",
      "[INFO] 'creating dataset\\testing\\I' directory\n",
      "[INFO] 'creating dataset\\testing\\O' directory\n",
      "[INFO] 'creating dataset\\testing\\X' directory\n",
      "[INFO] 'creating dataset\\testing\\B' directory\n",
      "[INFO] 'creating dataset\\testing\\U' directory\n",
      "[INFO] 'creating dataset\\testing\\Y' directory\n",
      "[INFO] 'creating dataset\\testing\\H' directory\n",
      "[INFO] 'creating dataset\\testing\\F' directory\n"
     ]
    }
   ],
   "source": [
    "# define the datasets that we'll be building\n",
    "datasets = [\n",
    "    (\"training\", trainPaths, TRAIN_PATH),\n",
    "    (\"validation\", valPaths, VAL_PATH),\n",
    "    (\"testing\", testPaths, TEST_PATH)\n",
    "]\n",
    "\n",
    "# loop over the datasets\n",
    "for (dType, imagePaths, baseOutput) in datasets:\n",
    "    # show which data split we are creating\n",
    "    print(f\"[INFO] building '{dType}' split\")\n",
    "\n",
    "    # if the output base output directory does not exist, create it\n",
    "    if not os.path.exists(baseOutput):\n",
    "        print(f\"[INFO] creating '{baseOutput}' directory\")\n",
    "        os.makedirs(baseOutput)\n",
    "\n",
    "    # loop over the input image paths\n",
    "    for inputPath in imagePaths:\n",
    "        # extract the filename of the input image along with its\n",
    "        # corresponding class label\n",
    "        filename = inputPath.split(os.path.sep)[-1]\n",
    "        label = inputPath.split(os.path.sep)[-2]\n",
    "\n",
    "        # build the path to the label directory\n",
    "        labelPath = os.path.sep.join([baseOutput, label])\n",
    "\n",
    "        # if the label output directory does not exist, create it\n",
    "        if not os.path.exists(labelPath):\n",
    "            print(f\"[INFO] creating '{labelPath}' directory\")\n",
    "            os.makedirs(labelPath)\n",
    "\n",
    "        # construct the path to the destination image and then copy\n",
    "        # the image itself\n",
    "        p = os.path.sep.join([labelPath, filename])\n",
    "        shutil.copy2(inputPath, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL - Compress them for Colab Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ARCHIVE_PATH = os.path.join(DATASET_DIR, \"archive.tar.gz\")\n",
    "\n",
    "# add the training and testing datasets to a tar file\n",
    "# uncomment this two lines below to do so\n",
    "# !tar -czf {ARCHIVE_PATH} {TRAIN_PATH} {VAL_PATH} {TEST_PATH}\n",
    "# print(f\"File saved at {ARCHIVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the YouTube video below for the inspiring this notebook.\n",
    "1. 36 Building Your Own Gesture Recognition System with Your Own Data - [YouTube Video](https://www.youtube.com/watch?v=YjnGou4skGU)\n",
    "2. PyImageSearch - [Fine-tuning ResNet with Keras, TensorFlow, and Deep Learning](https://www.pyimagesearch.com/2020/04/27/fine-tuning-resnet-with-keras-tensorflow-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
